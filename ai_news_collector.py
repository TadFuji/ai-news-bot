#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»ç¿»è¨³ãƒœãƒƒãƒˆ

24æ™‚é–“ä»¥å†…ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰åé›†ã—ã€
Gemini APIã§æ—¥æœ¬èªã«ç¿»è¨³ãƒ»è¦ç´„ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    export GOOGLE_API_KEY="your-api-key"
    python ai_news_collector.py
"""

import os
import json
from datetime import datetime, timedelta, timezone
import feedparser
from dateutil import parser as date_parser
import google.generativeai as genai

# ===========================
# è¨­å®š
# ===========================

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆAIé–¢é€£ã®ãƒ†ãƒƒã‚¯ãƒ¡ãƒ‡ã‚£ã‚¢ï¼‰
RSS_FEEDS = [
    {
        "name": "TechCrunch AI",
        "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
        "region": "ç±³å›½"
    },
    {
        "name": "The Verge AI",
        "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
        "region": "ç±³å›½"
    },
    {
        "name": "Wired AI",
        "url": "https://www.wired.com/feed/tag/ai/latest/rss",
        "region": "ç±³å›½"
    },
    {
        "name": "VentureBeat AI",
        "url": "https://venturebeat.com/category/ai/feed/",
        "region": "ç±³å›½"
    },
    {
        "name": "NHK ç§‘å­¦ãƒ»æŠ€è¡“",
        "url": "https://www.nhk.or.jp/rss/news/cat6.xml",
        "region": "æ—¥æœ¬"
    },
    {
        "name": "MIT Technology Review",
        "url": "https://www.technologyreview.com/feed/",
        "region": "ç±³å›½"
    },
    {
        "name": "Ars Technica AI",
        "url": "https://feeds.arstechnica.com/arstechnica/technology-lab",
        "region": "ç±³å›½"
    },
]

# AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
AI_KEYWORDS = [
    "AI", "artificial intelligence", "machine learning", "deep learning",
    "LLM", "GPT", "ChatGPT", "Gemini", "Claude", "OpenAI", "Anthropic",
    "neural network", "transformer", "generative AI", "ç”ŸæˆAI",
    "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°",
    "Copilot", "Midjourney", "Stable Diffusion", "DALL-E",
    "AGI", "superintelligence", "AI regulation", "AI ethics",
]

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, f"ai_news_{datetime.now().strftime('%Y%m%d_%H%M')}.md")

# ===========================
# ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†æ©Ÿèƒ½
# ===========================

def collect_from_rss_feeds() -> list[dict]:
    """
    è¤‡æ•°ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åé›†ã™ã‚‹
    
    Returns:
        è¨˜äº‹æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å…¬é–‹æ—¥æ™‚ã€ã‚½ãƒ¼ã‚¹åï¼‰
    """
    articles = []
    
    for feed_info in RSS_FEEDS:
        print(f"ğŸ“¡ {feed_info['name']} ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(feed_info["url"])
            
            for entry in feed.entries:
                # å…¬é–‹æ—¥æ™‚ã‚’å–å¾—ï¼ˆpublished ã¾ãŸã¯ updatedï¼‰
                pub_date = None
                if hasattr(entry, "published"):
                    pub_date = entry.published
                elif hasattr(entry, "updated"):
                    pub_date = entry.updated
                
                # æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                parsed_date = None
                if pub_date:
                    try:
                        parsed_date = date_parser.parse(pub_date)
                        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãŒãªã„å ´åˆã¯UTCã¨ã—ã¦æ‰±ã†
                        if parsed_date.tzinfo is None:
                            parsed_date = parsed_date.replace(tzinfo=timezone.utc)
                    except Exception:
                        pass
                
                # æ¦‚è¦ã‚’å–å¾—
                summary = ""
                if hasattr(entry, "summary"):
                    summary = entry.summary
                elif hasattr(entry, "description"):
                    summary = entry.description
                
                articles.append({
                    "title": entry.title if hasattr(entry, "title") else "No Title",
                    "url": entry.link if hasattr(entry, "link") else "",
                    "published": parsed_date,
                    "summary": summary[:500] if summary else "",  # æœ€å¤§500æ–‡å­—
                    "source": feed_info["name"],
                    "region": feed_info["region"],
                })
                
        except Exception as e:
            print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(f"âœ… åˆè¨ˆ {len(articles)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return articles


def filter_by_time(articles: list[dict]) -> list[dict]:
    """
    å‰æ—¥7æ™‚ã€œå½“æ—¥7æ™‚ï¼ˆJSTï¼‰ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        articles: è¨˜äº‹ãƒªã‚¹ãƒˆ
    
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    # JST (UTC+9) ã§ 7:00 ã‚’åŸºæº–ã«ã™ã‚‹
    jst = timezone(timedelta(hours=9))
    now_jst = datetime.now(jst)
    
    # å½“æ—¥ã®7:00 JST
    today_7am_jst = now_jst.replace(hour=7, minute=0, second=0, microsecond=0)
    
    # ã‚‚ã—ç¾åœ¨æ™‚åˆ»ãŒ7æ™‚ã‚ˆã‚Šå‰ãªã‚‰ã€åŸºæº–ã¯æ˜¨æ—¥ã®7æ™‚ã€œä»Šæ—¥ã®7æ™‚
    # ã‚‚ã—ç¾åœ¨æ™‚åˆ»ãŒ7æ™‚ä»¥é™ãªã‚‰ã€åŸºæº–ã¯ä»Šæ—¥ã®7æ™‚ã€œæ˜æ—¥ã®7æ™‚
    if now_jst.hour < 7:
        end_time = today_7am_jst
        start_time = end_time - timedelta(days=1)
    else:
        start_time = today_7am_jst
        end_time = start_time + timedelta(days=1)
    
    # UTC ã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
    start_time_utc = start_time.astimezone(timezone.utc)
    end_time_utc = end_time.astimezone(timezone.utc)
    
    filtered = []
    for article in articles:
        if article["published"]:
            pub_time = article["published"]
            if start_time_utc <= pub_time < end_time_utc:
                filtered.append(article)
    
    start_str = start_time.strftime('%m/%d %H:%M')
    end_str = end_time.strftime('%m/%d %H:%M')
    print(f"ğŸ“… {start_str} ã€œ {end_str} (JST) ã®è¨˜äº‹: {len(filtered)} ä»¶")
    return filtered


def filter_by_ai_keywords(articles: list[dict]) -> list[dict]:
    """
    AIé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¨˜äº‹ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        articles: è¨˜äº‹ãƒªã‚¹ãƒˆ
    
    Returns:
        AIé–¢é€£ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    filtered = []
    for article in articles:
        text = f"{article['title']} {article['summary']}".lower()
        for keyword in AI_KEYWORDS:
            if keyword.lower() in text:
                filtered.append(article)
                break
    
    print(f"ğŸ¤– AIé–¢é€£ã®è¨˜äº‹: {len(filtered)} ä»¶")
    return filtered


def remove_duplicates(articles: list[dict]) -> list[dict]:
    """
    é‡è¤‡ã™ã‚‹è¨˜äº‹ã‚’å‰Šé™¤ã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã§ãƒã‚§ãƒƒã‚¯ï¼‰
    
    Args:
        articles: è¨˜äº‹ãƒªã‚¹ãƒˆ
    
    Returns:
        é‡è¤‡ã‚’é™¤å»ã—ãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    seen_titles = set()
    unique = []
    
    for article in articles:
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ç©ºç™½é™¤å»ï¼‰
        normalized = article["title"].lower().strip()
        
        # æ—¢ã«åŒã˜ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if normalized not in seen_titles:
            seen_titles.add(normalized)
            unique.append(article)
    
    print(f"ğŸ“‹ é‡è¤‡é™¤å»å¾Œ: {len(unique)} ä»¶")
    return unique


# ===========================
# Gemini API ã«ã‚ˆã‚‹å‡¦ç†
# ===========================

def process_with_gemini(articles: list[dict], max_articles: int = 10) -> list[dict]:
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹ã‚’ç¿»è¨³ãƒ»è¦ç´„ã—ã€é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸ã™ã‚‹
    
    Args:
        articles: è¨˜äº‹ãƒªã‚¹ãƒˆ
        max_articles: å‡¦ç†ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°
    
    Returns:
        å‡¦ç†æ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆï¼ˆæ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã€æ—¥æœ¬èªè¦ç´„ã€ã‚¹ã‚³ã‚¢ä»˜ãï¼‰
    """
    # APIã‚­ãƒ¼ã‚’å–å¾—
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return articles[:max_articles]
    
    # Geminiã‚’è¨­å®š
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.0-flash")
    
    # è¨˜äº‹æƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    articles_text = ""
    for i, article in enumerate(articles[:30]):  # æœ€å¤§30ä»¶ã‚’å‡¦ç†å¯¾è±¡
        articles_text += f"""
---
è¨˜äº‹{i+1}:
ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}
ã‚½ãƒ¼ã‚¹: {article['source']} ({article['region']})
æ¦‚è¦: {article['summary'][:300]}
URL: {article['url']}
"""
    
    prompt = f"""ã‚ãªãŸã¯AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼åˆ†é‡ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã§å½±éŸ¿åŠ›ã®ã‚ã‚‹10ä»¶ã‚’é¸ã³ã€æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

é¸å®šåŸºæº–:
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå½±éŸ¿åº¦ï¼ˆæ”¿ç­–ã€ãƒ“ã‚¸ãƒã‚¹ã€æŠ€è¡“é©æ–°ï¼‰
- AIåˆ†é‡ã«ãŠã‘ã‚‹é‡è¦æ€§
- æ—¥æœ¬ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã¸ã®é–¢é€£æ€§
- é‡è¤‡ã™ã‚‹å†…å®¹ã¯1ã¤ã ã‘é¸ã¶

å‡ºåŠ›å½¢å¼ï¼ˆJSONé…åˆ—ï¼‰:
[
  {{
    "index": å…ƒã®è¨˜äº‹ç•ªå·,
    "title_ja": "æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«",
    "summary_ja": "2ã€œ3æ–‡ã®æ—¥æœ¬èªè¦ç´„ã€‚ãƒ“ã‚¸ãƒã‚¹å°‚é–€å®¶å‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ã",
    "importance_score": 1-10ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢,
    "reason": "é¸å®šç†ç”±ï¼ˆ1æ–‡ï¼‰"
  }},
  ...
]

---
è¨˜äº‹ãƒªã‚¹ãƒˆ:
{articles_text}
---

é‡è¦: å¿…ãš10ä»¶é¸ã³ã€JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¸è¦ã§ã™ã€‚
"""
    
    print("ğŸ§  Gemini API ã§å‡¦ç†ä¸­...")
    
    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯é™¤å»ï¼‰
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            response_text = "\n".join(lines[1:-1])
        
        results = json.loads(response_text)
        
        # çµæœã‚’å…ƒã®è¨˜äº‹æƒ…å ±ã¨ãƒãƒ¼ã‚¸
        processed = []
        for result in results:
            idx = result.get("index", 1) - 1
            if 0 <= idx < len(articles):
                article = articles[idx].copy()
                article["title_ja"] = result.get("title_ja", article["title"])
                article["summary_ja"] = result.get("summary_ja", "è¦ç´„ãªã—")
                article["importance_score"] = result.get("importance_score", 5)
                article["reason"] = result.get("reason", "")
                processed.append(article)
        
        # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        processed.sort(key=lambda x: x.get("importance_score", 0), reverse=True)
        
        print(f"âœ… Gemini å‡¦ç†å®Œäº†: {len(processed)} ä»¶")
        return processed[:max_articles]
        
    except Exception as e:
        print(f"âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®è¨˜äº‹ã‚’ãã®ã¾ã¾è¿”ã™
        return articles[:max_articles]


# ===========================
# å‡ºåŠ›æ©Ÿèƒ½
# ===========================

def output_markdown(articles: list[dict]) -> str:
    """
    è¨˜äº‹ãƒªã‚¹ãƒˆã‚’Markdownå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
    
    Args:
        articles: å‡¦ç†æ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆ
    
    Returns:
        Markdownå½¢å¼ã®æ–‡å­—åˆ—
    """
    now = datetime.now(timezone(timedelta(hours=9)))  # JST
    
    md = f"""# AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ TOP10

**æ›´æ–°æ—¥æ™‚**: {now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} (JST)

éå»24æ™‚é–“ä»¥å†…ã«å…¬é–‹ã•ã‚ŒãŸã€æœ€ã‚‚é‡è¦ãªAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚

---

"""
    
    for i, article in enumerate(articles, 1):
        title = article.get("title_ja", article["title"])
        summary = article.get("summary_ja", article.get("summary", "è¦ç´„ãªã—"))
        url = article["url"]
        source = article["source"]
        
        md += f"""## {i}. {title}

{summary}

- **å‡ºå…¸**: {source}
- **URL**: {url}

---

"""
    
    if not articles:
        md += "è©²å½“ã™ã‚‹AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯éå»24æ™‚é–“ä»¥å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
    
    return md


def save_output(content: str, filepath: str) -> None:
    """
    ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
    
    Args:
        content: ä¿å­˜ã™ã‚‹å†…å®¹
        filepath: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    
    print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {filepath}")


# ===========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===========================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 50)
    print("ğŸ¤– AI ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»ç¿»è¨³ãƒœãƒƒãƒˆ")
    print("=" * 50)
    print()
    
    # 1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†
    articles = collect_from_rss_feeds()
    
    if not articles:
        print("âŒ è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # 2. JST 7æ™‚åŸºæº–ã§24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    articles = filter_by_time(articles)
    
    # 3. AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    articles = filter_by_ai_keywords(articles)
    
    # 4. é‡è¤‡ã‚’é™¤å»
    articles = remove_duplicates(articles)
    
    if not articles:
        print("âš ï¸ éå»24æ™‚é–“ä»¥å†…ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        # ç©ºã®å‡ºåŠ›ã‚’ç”Ÿæˆ
        md = output_markdown([])
        save_output(md, OUTPUT_FILE)
        print(md)
        return
    
    # 5. Gemini APIã§ç¿»è¨³ãƒ»è¦ç´„ãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    processed = process_with_gemini(articles, max_articles=10)
    
    # 6. Markdownå‡ºåŠ›
    md = output_markdown(processed)
    
    # 7. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    save_output(md, OUTPUT_FILE)
    
    # 8. æ¨™æº–å‡ºåŠ›ã«ã‚‚è¡¨ç¤ºï¼ˆGitHub Actions ã®ãƒ­ã‚°ç”¨ï¼‰
    print()
    print("=" * 50)
    print("ğŸ“° å‡ºåŠ›çµæœ")
    print("=" * 50)
    print()
    print(md)


if __name__ == "__main__":
    main()
