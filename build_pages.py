#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Pages ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

output/ å†…ã® Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€
GitHub Pages ç”¨ã® JSON ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import os
import re
import json
import shutil
from pathlib import Path
from config import NEWS_BOT_OUTPUT_DIR as output_dir_path


def parse_markdown_news(content: str) -> dict:
    """
    Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        content: Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
    
    Returns:
        ãƒ‘ãƒ¼ã‚¹çµæœï¼ˆæ›´æ–°æ—¥æ™‚ã€è¨˜äº‹ãƒªã‚¹ãƒˆï¼‰
    """
    result = {
        "updated": "",
        "articles": []
    }
    
    # æ›´æ–°æ—¥æ™‚ã‚’æŠ½å‡º
    date_match = re.search(r'\*\*æ›´æ–°æ—¥æ™‚\*\*:\s*(.+?)(?:\s*\(JST\))?$', content, re.MULTILINE)
    if date_match:
        result["updated"] = date_match.group(1).strip()
    
    # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆ## 1. ã‚¿ã‚¤ãƒˆãƒ« å½¢å¼ï¼‰
    article_pattern = re.compile(
        r'##\s*\d+\.\s*(.+?)\n\n(.+?)\n\n-\s*\*\*å‡ºå…¸\*\*:\s*(.+?)\n-\s*\*\*URL\*\*:\s*(.+?)(?:\n|$)',
        re.DOTALL
    )
    
    for match in article_pattern.finditer(content):
        result["articles"].append({
            "title": match.group(1).strip(),
            "summary": match.group(2).strip(),
            "source": match.group(3).strip(),
            "url": match.group(4).strip()
        })
    
    return result


def build_pages():
    """
    GitHub Pages ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    script_dir = Path(__file__).parent
    output_dir = Path(output_dir_path)
    docs_dir = script_dir / "docs"
    
    # docs ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºä¿
    docs_dir.mkdir(exist_ok=True)
    
    # output å†…ã®å…¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    md_files = sorted(output_dir.glob("ai_news_*.md"), reverse=True)
    
    if not md_files:
        print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    archives = []
    
    for md_file in md_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆai_news_20260116_1453.mdï¼‰
        match = re.search(r'ai_news_(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})', md_file.name)
        if not match:
            continue
        
        year, month, day, hour, minute = match.groups()
        date_str = f"{year}å¹´{month}æœˆ{day}æ—¥"
        file_date = f"{year}-{month}-{day}"
        
        # Markdown ã‚’è§£æ
        content = md_file.read_text(encoding="utf-8")
        parsed = parse_markdown_news(content)
        
        if not parsed["articles"]:
            continue
        
        # æ—¥ä»˜åˆ¥ JSON ã‚’ä¿å­˜
        date_json_path = docs_dir / f"{file_date}.json"
        with open(date_json_path, "w", encoding="utf-8") as f:
            json.dump(parsed, f, ensure_ascii=False, indent=2)
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã«è¿½åŠ 
        archives.append({
            "date": date_str,
            "path": f"{file_date}.json",
            "count": len(parsed["articles"])
        })
        
        print(f"âœ… {md_file.name} â†’ {file_date}.json ({len(parsed['articles'])} ä»¶)")
    
    # æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ latest.json ã¨ã—ã¦ä¿å­˜
    if archives:
        latest_date = archives[0]["path"]
        latest_json_path = docs_dir / latest_date
        if latest_json_path.exists():
            shutil.copy(latest_json_path, docs_dir / "latest.json")
            print("âœ… latest.json æ›´æ–°")
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã‚’æ—¥ä»˜ã§é‡è¤‡æ’é™¤ï¼ˆåŒä¸€æ—¥ä»˜ã¯æœ€æ–°ã®ã‚‚ã®ã®ã¿ä¿æŒï¼‰
    seen_dates = set()
    unique_archives = []
    for archive in archives:
        if archive["path"] not in seen_dates:
            seen_dates.add(archive["path"])
            unique_archives.append(archive)
    
    archive_data = {"archives": unique_archives}
    with open(docs_dir / "archive.json", "w", encoding="utf-8") as f:
        json.dump(archive_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… archive.json æ›´æ–° ({len(unique_archives)} ä»¶)")

    # --- Column Processing ---
    column_dir = output_dir.parent / "output" / "columns" # Correct path based on structure
    if not column_dir.exists():
        column_dir = output_dir / "columns" # Fallback check

    columns_files = sorted(column_dir.glob("weekly_column_*.md"), reverse=True)
    columns_list = []

    for cfile in columns_files:
        # Patter: weekly_column_YYYYMMDD.md
        match = re.search(r'weekly_column_(\d{4})(\d{2})(\d{2})', cfile.name)
        if not match:
            continue
        
        y, m, d = match.groups()
        date_display = f"{y}å¹´{m}æœˆ{d}æ—¥"
        
        content = cfile.read_text(encoding="utf-8")
        
        # Simple parse: Title is line 1, Body is rest
        lines = content.split('\n')
        title = lines[0].replace('# ', '').strip()
        body = "\n".join(lines[1:]).strip()
        
        # Save individual JSON
        c_json_path = docs_dir / f"column_{y}{m}{d}.json"
        with open(c_json_path, "w", encoding="utf-8") as f:
            json.dump({"title": title, "date": date_display, "body": body}, f, ensure_ascii=False, indent=2)
            
        columns_list.append({
            "date": date_display,
            "title": title,
            "path": f"column_{y}{m}{d}.json"
        })
        print(f"âœ… Column Processed: {cfile.name}")

    with open(docs_dir / "columns.json", "w", encoding="utf-8") as f:
        json.dump({"columns": columns_list}, f, ensure_ascii=False, indent=2)
    print(f"âœ… columns.json æ›´æ–° ({len(columns_list)} ä»¶)")

    print("ğŸ‰ ãƒ“ãƒ«ãƒ‰å®Œäº†!")


if __name__ == "__main__":
    build_pages()
