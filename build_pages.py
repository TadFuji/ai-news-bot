#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub Pages ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

output/ å†…ã® Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€
GitHub Pages ç”¨ã® JSON ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import os
import re
import json
from datetime import datetime
from pathlib import Path


def parse_markdown_news(content: str) -> dict:
    """
    Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        content: Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
    
    Returns:
        ãƒ‘ãƒ¼ã‚¹çµæœï¼ˆæ›´æ–°æ—¥æ™‚ã€è¨˜äº‹ãƒªã‚¹ãƒˆï¼‰
    """
    result = {
        "updated": "",
        "articles": []
    }
    
    # æ›´æ–°æ—¥æ™‚ã‚’æŠ½å‡º
    date_match = re.search(r'\*\*æ›´æ–°æ—¥æ™‚\*\*:\s*(.+?)(?:\s*\(JST\))?$', content, re.MULTILINE)
    if date_match:
        result["updated"] = date_match.group(1).strip()
    
    # è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆ## 1. ã‚¿ã‚¤ãƒˆãƒ« å½¢å¼ï¼‰
    article_pattern = re.compile(
        r'##\s*\d+\.\s*(.+?)\n\n(.+?)\n\n-\s*\*\*å‡ºå…¸\*\*:\s*(.+?)\n-\s*\*\*URL\*\*:\s*(.+?)(?:\n|$)',
        re.DOTALL
    )
    
    for match in article_pattern.finditer(content):
        result["articles"].append({
            "title": match.group(1).strip(),
            "summary": match.group(2).strip(),
            "source": match.group(3).strip(),
            "url": match.group(4).strip()
        })
    
    return result


def build_pages():
    """
    GitHub Pages ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    script_dir = Path(__file__).parent
    output_dir = script_dir / "output"
    docs_dir = script_dir / "docs"
    
    # docs ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºä¿
    docs_dir.mkdir(exist_ok=True)
    
    # output å†…ã®å…¨ Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    md_files = sorted(output_dir.glob("ai_news_*.md"), reverse=True)
    
    if not md_files:
        print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    archives = []
    
    for md_file in md_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆai_news_20260116_1453.mdï¼‰
        match = re.search(r'ai_news_(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})', md_file.name)
        if not match:
            continue
        
        year, month, day, hour, minute = match.groups()
        date_str = f"{year}å¹´{month}æœˆ{day}æ—¥"
        file_date = f"{year}-{month}-{day}"
        
        # Markdown ã‚’è§£æ
        content = md_file.read_text(encoding="utf-8")
        parsed = parse_markdown_news(content)
        
        if not parsed["articles"]:
            continue
        
        # æ—¥ä»˜åˆ¥ JSON ã‚’ä¿å­˜
        date_json_path = docs_dir / f"{file_date}.json"
        with open(date_json_path, "w", encoding="utf-8") as f:
            json.dump(parsed, f, ensure_ascii=False, indent=2)
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã«è¿½åŠ 
        archives.append({
            "date": date_str,
            "path": f"{file_date}.json",
            "count": len(parsed["articles"])
        })
        
        print(f"âœ… {md_file.name} â†’ {file_date}.json ({len(parsed['articles'])} ä»¶)")
    
    # æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ latest.json ã¨ã—ã¦ä¿å­˜
    if archives:
        latest_date = archives[0]["path"]
        latest_json_path = docs_dir / latest_date
        if latest_json_path.exists():
            import shutil
            shutil.copy(latest_json_path, docs_dir / "latest.json")
            print(f"âœ… latest.json æ›´æ–°")
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã‚’ä¿å­˜
    archive_data = {"archives": archives}
    with open(docs_dir / "archive.json", "w", encoding="utf-8") as f:
        json.dump(archive_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… archive.json æ›´æ–° ({len(archives)} ä»¶)")
    print("ğŸ‰ ãƒ“ãƒ«ãƒ‰å®Œäº†!")


if __name__ == "__main__":
    build_pages()
